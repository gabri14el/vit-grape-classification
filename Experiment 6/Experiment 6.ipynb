{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[ViT]  - Experimento 6",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7W7v1i4gVn4"
      },
      "source": [
        "Baixando e extraindo o dataset:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive._mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyTsMj2Jt086",
        "outputId": "e967a365-3c81-4508-a188-21974809f736"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXEKzdpBiCgN",
        "outputId": "fd0f31f5-94c8-44f6-e636-574eef170277"
      },
      "source": [
        "!rm -r dataset_aumentado/\n",
        "!unzip -o -q /content/gdrive/MyDrive/dataset_aumentado_384.zip"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'dataset_aumentado/': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9T4CFWtdR8r"
      },
      "source": [
        "Importando as bibliotecas necessÃ¡rias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qveJTWbdV3z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0049db0-90c2-4188-a9a9-d48fa100ceca"
      },
      "source": [
        "!pip install tensorflow_addons\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import glob, warnings\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "print('TensorFlow Version ' + tf.__version__)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.7/dist-packages (0.15.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "TensorFlow Version 2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 384\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 100\n",
        "\n",
        "diretorio = 'dataset_aumentado_384/images'\n",
        "\n",
        "TRAIN_PATH = os.path.join(diretorio, 'train')\n",
        "TEST_PATH = os.path.join(diretorio, 'test')\n",
        "VALIDATION_PATH = os.path.join(diretorio, 'validation')\n",
        "\n",
        "classes={\n",
        "    0:'CD',\n",
        "    1:'MF',\n",
        "    2:'MP',\n",
        "    3:'MR',\n",
        "    4:'MG',\n",
        "    5:'MT',\n",
        "    6:'RG',\n",
        "    7:'TA',\n",
        "    8:'TB',\n",
        "    9:'TR',\n",
        "    10:'TC',\n",
        "    11:'TN'\n",
        "}"
      ],
      "metadata": {
        "id": "RwM6JCdbiZQ0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras.backend as K\n",
        "import math\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "#------------------------\n",
        "#step decay para o SGD\n",
        "def step_decay1(epoch):\n",
        "   initial_lrate = 1e-4\n",
        "   flattern_factor = initial_lrate ** 2.25\n",
        "   epochs_drop = 5.0\n",
        "   #drop modelado como modelado no artigo\n",
        "   drop = initial_lrate **(flattern_factor/epochs_drop)\n",
        "   \n",
        "   lrate = initial_lrate * math.pow(drop,  \n",
        "           math.floor((epoch)/epochs_drop))\n",
        "   return lrate\n",
        "\n",
        "#step decay\n",
        "def step_decay2(epoch):\n",
        "   initial_lrate = 1e-6\n",
        "   flattern_factor = initial_lrate ** 2.25\n",
        "   epochs_drop = 5.0\n",
        "   #drop modelado como modelado no artigo\n",
        "   drop = initial_lrate **(flattern_factor/epochs_drop)\n",
        "   \n",
        "   lrate = initial_lrate * math.pow(drop,  \n",
        "           math.floor((epoch)/epochs_drop))\n",
        "   return lrate"
      ],
      "metadata": {
        "id": "J1PpkIdmvyBA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet vit-keras\n",
        "\n",
        "from vit_keras import vit"
      ],
      "metadata": {
        "id": "cxyZvmvjlIts"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=vit.preprocess_inputs)\n",
        "\n",
        "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=vit.preprocess_inputs)\n",
        "\n",
        "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=vit.preprocess_inputs)\n",
        "\n",
        "train_set = train_datagen.flow_from_directory(TRAIN_PATH,\n",
        "                                              target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "                                              class_mode='categorical',\n",
        "                                              batch_size=BATCH_SIZE)\n",
        "\n",
        "val_set = train_datagen.flow_from_directory(VALIDATION_PATH,\n",
        "                                              target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "                                              class_mode='categorical',\n",
        "                                              batch_size=BATCH_SIZE)\n",
        "\n",
        "test_set = test_datagen.flow_from_directory(TEST_PATH, \n",
        "                                            target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "                                            class_mode='categorical',\n",
        "                                            batch_size=1,\n",
        "                                            shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keKbeo__kZDa",
        "outputId": "2d7a2d08-410b-4d3b-924c-6dd426257939"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6718 images belonging to 12 classes.\n",
            "Found 132 images belonging to 12 classes.\n",
            "Found 72 images belonging to 12 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vit_model = vit.vit_b16(\n",
        "        image_size = IMAGE_SIZE,\n",
        "        activation = 'softmax',\n",
        "        pretrained = True,\n",
        "        include_top = False,\n",
        "        pretrained_top = False,\n",
        "        classes = len(classes))"
      ],
      "metadata": {
        "id": "Qrmd28sIllkq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv2D, Dense, MaxPooling2D, Flatten, Dropout, BatchNormalization, GlobalMaxPooling2D\n",
        "\n",
        "x = tf.keras.layers.Flatten()(vit_model.output)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.25)(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.25)(x)\n",
        "output = Dense(len(classes), activation='softmax')(x)\n",
        "model = tf.keras.models.Model(vit_model.input, output) \n",
        "\n",
        "\n",
        "for layer in range(len(vit_model.layers)):\n",
        "    vit_model.layers[layer].treinable=False"
      ],
      "metadata": {
        "id": "-_MyJ9lmCk6A"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-4\n",
        "\n",
        "optimizer = tfa.optimizers.RectifiedAdam(learning_rate = learning_rate)\n",
        "\n",
        "model.compile(optimizer = optimizer, \n",
        "              loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing = 0.2), \n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "\n",
        "STEP_SIZE_TRAIN = train_set.n // train_set.batch_size\n",
        "STEP_SIZE_VALID = val_set.n // val_set.batch_size\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_accuracy',\n",
        "                                                 factor = 0.2,\n",
        "                                                 patience = 2,\n",
        "                                                 verbose = 1,\n",
        "                                                 min_delta = 1e-4,\n",
        "                                                 min_lr = 1e-6,\n",
        "                                                 mode = 'max')\n",
        "\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy',\n",
        "                                                 min_delta = 1e-4,\n",
        "                                                 patience = 3,\n",
        "                                                 mode = 'max',\n",
        "                                                 restore_best_weights = True,\n",
        "                                                 verbose = 1)\n",
        "\n",
        "checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath = '/content/gdrive/MyDrive/model_best_7.h5',\n",
        "                                                  monitor = 'val_accuracy', \n",
        "                                                  verbose = 1, \n",
        "                                                  save_best_only = True,\n",
        "                                                  save_weights_only = True,\n",
        "                                                  mode = 'max')\n",
        "\n",
        "callbacks = [earlystopping, reduce_lr, checkpointer]\n",
        "\n",
        "model.fit(x = train_set,\n",
        "          steps_per_epoch = STEP_SIZE_TRAIN,\n",
        "          validation_data = val_set,\n",
        "          validation_steps = STEP_SIZE_VALID,\n",
        "          epochs = EPOCHS,\n",
        "          callbacks = callbacks)\n",
        "\n",
        "#model.save('model.h5', save_weights_only = True)\n",
        "\n",
        "model.load_weights('/content/gdrive/MyDrive/model_best_7.h5')\n",
        "\n",
        "learning_rate = 1e-6\n",
        "\n",
        "for layer in range(len(vit_model.layers)):\n",
        "    vit_model.layers[layer].treinable=True\n",
        "  \n",
        "\n",
        "model.compile(optimizer = optimizer, \n",
        "              loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing = 0.2), \n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(x = train_set,\n",
        "          steps_per_epoch = STEP_SIZE_TRAIN,\n",
        "          validation_data = val_set,\n",
        "          validation_steps = STEP_SIZE_VALID,\n",
        "          epochs = EPOCHS,\n",
        "          callbacks = callbacks)\n",
        "\n",
        "model.load_weights('/content/gdrive/MyDrive/model_best_7.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqJvsgW5Cv0Y",
        "outputId": "ab1eff88-551b-4f9d-f4f2-dc9553c0a30e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "839/839 [==============================] - ETA: 0s - loss: 1.5335 - accuracy: 0.7115\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.89062, saving model to /content/gdrive/MyDrive/model_best_7.h5\n",
            "839/839 [==============================] - 1014s 1s/step - loss: 1.5335 - accuracy: 0.7115 - val_loss: 1.1780 - val_accuracy: 0.8906 - lr: 1.0000e-04\n",
            "Epoch 2/100\n",
            "839/839 [==============================] - ETA: 0s - loss: 1.0299 - accuracy: 0.9811\n",
            "Epoch 00002: val_accuracy did not improve from 0.89062\n",
            "839/839 [==============================] - 986s 1s/step - loss: 1.0299 - accuracy: 0.9811 - val_loss: 1.1145 - val_accuracy: 0.8750 - lr: 1.0000e-04\n",
            "Epoch 3/100\n",
            "839/839 [==============================] - ETA: 0s - loss: 1.0293 - accuracy: 0.9687\n",
            "Epoch 00003: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.89062\n",
            "839/839 [==============================] - 986s 1s/step - loss: 1.0293 - accuracy: 0.9687 - val_loss: 1.1740 - val_accuracy: 0.8750 - lr: 1.0000e-04\n",
            "Epoch 4/100\n",
            "839/839 [==============================] - ETA: 0s - loss: 0.9557 - accuracy: 0.9987\n",
            "Epoch 00004: val_accuracy improved from 0.89062 to 0.93750, saving model to /content/gdrive/MyDrive/model_best_7.h5\n",
            "839/839 [==============================] - 992s 1s/step - loss: 0.9557 - accuracy: 0.9987 - val_loss: 1.0367 - val_accuracy: 0.9375 - lr: 2.0000e-05\n",
            "Epoch 5/100\n",
            "839/839 [==============================] - ETA: 0s - loss: 0.9463 - accuracy: 0.9999\n",
            "Epoch 00005: val_accuracy did not improve from 0.93750\n",
            "839/839 [==============================] - 985s 1s/step - loss: 0.9463 - accuracy: 0.9999 - val_loss: 1.0361 - val_accuracy: 0.9375 - lr: 2.0000e-05\n",
            "Epoch 6/100\n",
            "839/839 [==============================] - ETA: 0s - loss: 0.9429 - accuracy: 1.0000\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.93750\n",
            "839/839 [==============================] - 986s 1s/step - loss: 0.9429 - accuracy: 1.0000 - val_loss: 1.0316 - val_accuracy: 0.9375 - lr: 2.0000e-05\n",
            "Epoch 7/100\n",
            "839/839 [==============================] - ETA: 0s - loss: 0.9412 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 4.\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.93750\n",
            "839/839 [==============================] - 984s 1s/step - loss: 0.9412 - accuracy: 1.0000 - val_loss: 1.0336 - val_accuracy: 0.9375 - lr: 4.0000e-06\n",
            "Epoch 00007: early stopping\n",
            "Epoch 1/100\n",
            "839/839 [==============================] - ETA: 0s - loss: 0.9472 - accuracy: 1.0000\n",
            "Epoch 00001: val_accuracy improved from 0.93750 to 0.95312, saving model to /content/gdrive/MyDrive/model_best_7.h5\n",
            "839/839 [==============================] - 1011s 1s/step - loss: 0.9472 - accuracy: 1.0000 - val_loss: 1.0384 - val_accuracy: 0.9531 - lr: 4.0000e-06\n",
            "Epoch 2/100\n",
            "839/839 [==============================] - ETA: 0s - loss: 0.9444 - accuracy: 1.0000\n",
            "Epoch 00002: val_accuracy did not improve from 0.95312\n",
            "839/839 [==============================] - 984s 1s/step - loss: 0.9444 - accuracy: 1.0000 - val_loss: 1.0487 - val_accuracy: 0.9453 - lr: 4.0000e-06\n",
            "Epoch 3/100\n",
            "839/839 [==============================] - ETA: 0s - loss: 0.9435 - accuracy: 1.0000\n",
            "Epoch 00003: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.95312\n",
            "839/839 [==============================] - 984s 1s/step - loss: 0.9435 - accuracy: 1.0000 - val_loss: 1.0307 - val_accuracy: 0.9453 - lr: 4.0000e-06\n",
            "Epoch 4/100\n",
            "839/839 [==============================] - ETA: 0s - loss: 0.9419 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 1.\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.95312\n",
            "839/839 [==============================] - 982s 1s/step - loss: 0.9419 - accuracy: 1.0000 - val_loss: 1.0338 - val_accuracy: 0.9531 - lr: 1.0000e-06\n",
            "Epoch 00004: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes={\n",
        "    'Codega':'CD',\n",
        "    'Malvasia Fina':'MF',\n",
        "    'Malvasia Preta':'MP',\n",
        "    'Malvasia Rei':'MR',\n",
        "    'Moscatel Galego':'MG',\n",
        "    'Mourisco Tinto':'MT',\n",
        "    'Rabigato':'RG',\n",
        "    'Tinta Amarela':'TA',\n",
        "    'Tinta Barroca':'TB',\n",
        "    'Tinta Roriz':'TR',\n",
        "    'Tinto Cao':'TC',\n",
        "    'Touriga Nacional':'TN'\n",
        "}\n",
        "\n",
        "model.load_weights('/content/gdrive/MyDrive/model_best_7.h5')\n",
        "from sklearn import metrics\n",
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "def confusion_matrix(test_data_generator, model):\n",
        "  test_data_generator.reset()\n",
        "  predictions = model.predict(test_data_generator, steps=test_set.samples)\n",
        "  # Get most likely class\n",
        "  predicted_classes = np.argmax(predictions, axis=1)\n",
        "  true_classes = test_data_generator.classes\n",
        "  class_labels = list(test_data_generator.class_indices.keys())\n",
        "  print(class_labels)\n",
        "  class_labels = [classes[x] for x in class_labels]  \n",
        "\n",
        "  report = metrics.classification_report(true_classes, predicted_classes, target_names=class_labels)\n",
        "  cm = metrics.confusion_matrix(true_classes, predicted_classes)\n",
        "  print(report)\n",
        "  plot_confusion_matrix(cm, class_labels)\n",
        "\n",
        "\n",
        "confusion_matrix(test_set, model)"
      ],
      "metadata": {
        "id": "P03ty_iOuIXc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 948
        },
        "outputId": "17858351-f7a1-47e1-9842-9f4cc90a2213"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Codega', 'Malvasia Fina', 'Malvasia Preta', 'Malvasia Rei', 'Moscatel Galego', 'Mourisco Tinto', 'Rabigato', 'Tinta Amarela', 'Tinta Barroca', 'Tinta Roriz', 'Tinto Cao', 'Touriga Nacional']\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          CD       1.00      0.67      0.80         6\n",
            "          MF       1.00      0.83      0.91         6\n",
            "          MP       1.00      1.00      1.00         6\n",
            "          MR       0.67      1.00      0.80         6\n",
            "          MG       1.00      1.00      1.00         6\n",
            "          MT       1.00      1.00      1.00         6\n",
            "          RG       1.00      1.00      1.00         6\n",
            "          TA       1.00      1.00      1.00         6\n",
            "          TB       1.00      1.00      1.00         6\n",
            "          TR       1.00      1.00      1.00         6\n",
            "          TC       1.00      1.00      1.00         6\n",
            "          TN       1.00      1.00      1.00         6\n",
            "\n",
            "    accuracy                           0.96        72\n",
            "   macro avg       0.97      0.96      0.96        72\n",
            "weighted avg       0.97      0.96      0.96        72\n",
            "\n",
            "Confusion matrix, without normalization\n",
            "[[4 0 0 2 0 0 0 0 0 0 0 0]\n",
            " [0 5 0 1 0 0 0 0 0 0 0 0]\n",
            " [0 0 6 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 6 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 6 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 6 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 6 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 6 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 6 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 6 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 6 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 6]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dot_img_file = 'model_1.png'\n",
        "tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "YilmpsdwXhWI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9114d432-51fe-46a0-9a1a-e6dd90419046"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 384, 384, 3)]     0         \n",
            "                                                                 \n",
            " embedding (Conv2D)          (None, 24, 24, 768)       590592    \n",
            "                                                                 \n",
            " reshape_1 (Reshape)         (None, 576, 768)          0         \n",
            "                                                                 \n",
            " class_token (ClassToken)    (None, 577, 768)          768       \n",
            "                                                                 \n",
            " Transformer/posembed_input   (None, 577, 768)         443136    \n",
            " (AddPositionEmbs)                                               \n",
            "                                                                 \n",
            " Transformer/encoderblock_0   ((None, 577, 768),       7087872   \n",
            " (TransformerBlock)           (None, 12, None, None))            \n",
            "                                                                 \n",
            " Transformer/encoderblock_1   ((None, 577, 768),       7087872   \n",
            " (TransformerBlock)           (None, 12, None, None))            \n",
            "                                                                 \n",
            " Transformer/encoderblock_2   ((None, 577, 768),       7087872   \n",
            " (TransformerBlock)           (None, 12, None, None))            \n",
            "                                                                 \n",
            " Transformer/encoderblock_3   ((None, 577, 768),       7087872   \n",
            " (TransformerBlock)           (None, 12, None, None))            \n",
            "                                                                 \n",
            " Transformer/encoderblock_4   ((None, 577, 768),       7087872   \n",
            " (TransformerBlock)           (None, 12, None, None))            \n",
            "                                                                 \n",
            " Transformer/encoderblock_5   ((None, 577, 768),       7087872   \n",
            " (TransformerBlock)           (None, 12, None, None))            \n",
            "                                                                 \n",
            " Transformer/encoderblock_6   ((None, 577, 768),       7087872   \n",
            " (TransformerBlock)           (None, 12, None, None))            \n",
            "                                                                 \n",
            " Transformer/encoderblock_7   ((None, 577, 768),       7087872   \n",
            " (TransformerBlock)           (None, 12, None, None))            \n",
            "                                                                 \n",
            " Transformer/encoderblock_8   ((None, 577, 768),       7087872   \n",
            " (TransformerBlock)           (None, 12, None, None))            \n",
            "                                                                 \n",
            " Transformer/encoderblock_9   ((None, 577, 768),       7087872   \n",
            " (TransformerBlock)           (None, 12, None, None))            \n",
            "                                                                 \n",
            " Transformer/encoderblock_10  ((None, 577, 768),       7087872   \n",
            "  (TransformerBlock)          (None, 12, None, None))            \n",
            "                                                                 \n",
            " Transformer/encoderblock_11  ((None, 577, 768),       7087872   \n",
            "  (TransformerBlock)          (None, 12, None, None))            \n",
            "                                                                 \n",
            " Transformer/encoder_norm (L  (None, 577, 768)         1536      \n",
            " ayerNormalization)                                              \n",
            "                                                                 \n",
            " ExtractToken (Lambda)       (None, 768)               0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 768)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 512)               393728    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 512)               262656    \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 12)                6156      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 86,753,036\n",
            "Trainable params: 86,753,036\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}