{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7W7v1i4gVn4"
      },
      "source": [
        "Baixando e extraindo o dataset:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9T4CFWtdR8r"
      },
      "source": [
        "Importando as bibliotecas necessÃ¡rias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qveJTWbdV3z",
        "outputId": "c1f9556b-c9c9-4e8c-ab46-93d8b854a853"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.7/dist-packages (0.15.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "TensorFlow Version 2.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow_addons\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import glob, warnings\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "print('TensorFlow Version ' + tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyTsMj2Jt086",
        "outputId": "60e67a1c-56f4-41f1-bbf3-bcd64c93ff35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive._mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RwM6JCdbiZQ0"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 100\n",
        "\n",
        "diretorio = 'dataset_aumentado'\n",
        "\n",
        "TRAIN_PATH = os.path.join(diretorio, 'train')\n",
        "TEST_PATH = os.path.join(diretorio, 'test')\n",
        "VALIDATION_PATH = os.path.join(diretorio, 'validation')\n",
        "\n",
        "classes={\n",
        "    0:'CD',\n",
        "    1:'MF',\n",
        "    2:'MP',\n",
        "    3:'MR',\n",
        "    4:'MG',\n",
        "    5:'MT',\n",
        "    6:'RG',\n",
        "    7:'TA',\n",
        "    8:'TB',\n",
        "    9:'TR',\n",
        "    10:'TC',\n",
        "    11:'TN'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "cxyZvmvjlIts"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet vit-keras\n",
        "\n",
        "from vit_keras import vit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5qV4ZG1E77ko"
      },
      "outputs": [],
      "source": [
        "import tensorflow.keras.backend as K\n",
        "import math\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "#------------------------\n",
        "#step decay para o SGD\n",
        "def step_decay1(epoch):\n",
        "   initial_lrate = 1e-4\n",
        "   flattern_factor = initial_lrate ** 2.25\n",
        "   epochs_drop = 5.0\n",
        "   #drop modelado como modelado no artigo\n",
        "   drop = initial_lrate **(flattern_factor/epochs_drop)\n",
        "   \n",
        "   lrate = initial_lrate * math.pow(drop,  \n",
        "           math.floor((epoch)/epochs_drop))\n",
        "   return lrate\n",
        "\n",
        "#step decay\n",
        "def step_decay2(epoch):\n",
        "   initial_lrate = 1e-6\n",
        "   flattern_factor = initial_lrate ** 2.25\n",
        "   epochs_drop = 5.0\n",
        "   #drop modelado como modelado no artigo\n",
        "   drop = initial_lrate **(flattern_factor/epochs_drop)\n",
        "   \n",
        "   lrate = initial_lrate * math.pow(drop,  \n",
        "           math.floor((epoch)/epochs_drop))\n",
        "   return lrate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keKbeo__kZDa",
        "outputId": "0c88e1c3-760a-4464-ec70-783722060486"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 6718 images belonging to 12 classes.\n",
            "Found 132 images belonging to 12 classes.\n",
            "Found 72 images belonging to 12 classes.\n"
          ]
        }
      ],
      "source": [
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=vit.preprocess_inputs)\n",
        "\n",
        "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=vit.preprocess_inputs)\n",
        "\n",
        "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=vit.preprocess_inputs)\n",
        "\n",
        "train_set = train_datagen.flow_from_directory(TRAIN_PATH,\n",
        "                                              target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "                                              class_mode='categorical',\n",
        "                                              batch_size=BATCH_SIZE)\n",
        "\n",
        "val_set = train_datagen.flow_from_directory(VALIDATION_PATH,\n",
        "                                              target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "                                              class_mode='categorical',\n",
        "                                              batch_size=BATCH_SIZE)\n",
        "\n",
        "test_set = test_datagen.flow_from_directory(TEST_PATH, \n",
        "                                            target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "                                            class_mode='categorical',\n",
        "                                            batch_size=1,\n",
        "                                            shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Qrmd28sIllkq"
      },
      "outputs": [],
      "source": [
        "vit_model = vit.vit_b16(\n",
        "        image_size = IMAGE_SIZE,\n",
        "        activation = 'softmax',\n",
        "        pretrained = True,\n",
        "        include_top = False,\n",
        "        pretrained_top = False,\n",
        "        classes = len(classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "-_MyJ9lmCk6A"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv2D, Dense, MaxPooling2D, Flatten, Dropout, BatchNormalization, GlobalMaxPooling2D\n",
        "\n",
        "x = tf.keras.layers.Flatten()(vit_model.output)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.25)(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.25)(x)\n",
        "output = Dense(len(classes), activation='softmax')(x)\n",
        "model = tf.keras.models.Model(vit_model.input, output) \n",
        "\n",
        "\n",
        "for layer in range(len(vit_model.layers)):\n",
        "    vit_model.layers[layer].treinable=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqJvsgW5Cv0Y",
        "outputId": "67371593-ef5f-4964-c946-011c53040018"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "419/419 [==============================] - ETA: 0s - loss: 2.7575 - accuracy: 0.0861 - f1_m: 0.0020 - precision_m: 0.0167 - recall_m: 0.0010\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.10156, saving model to /content/gdrive/MyDrive/model_best11v2.h5\n",
            "419/419 [==============================] - 681s 2s/step - loss: 2.7575 - accuracy: 0.0861 - f1_m: 0.0020 - precision_m: 0.0167 - recall_m: 0.0010 - val_loss: 2.5009 - val_accuracy: 0.1016 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - lr: 1.0000e-04\n",
            "Epoch 2/100\n",
            "419/419 [==============================] - ETA: 0s - loss: 2.6627 - accuracy: 0.0997 - f1_m: 2.8078e-04 - precision_m: 0.0024 - recall_m: 1.4916e-04\n",
            "Epoch 00002: val_accuracy improved from 0.10156 to 0.13281, saving model to /content/gdrive/MyDrive/model_best11v2.h5\n",
            "419/419 [==============================] - 662s 2s/step - loss: 2.6627 - accuracy: 0.0997 - f1_m: 2.8078e-04 - precision_m: 0.0024 - recall_m: 1.4916e-04 - val_loss: 2.4505 - val_accuracy: 0.1328 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - lr: 1.0000e-04\n",
            "Epoch 3/100\n",
            "419/419 [==============================] - ETA: 0s - loss: 2.6025 - accuracy: 0.1109 - f1_m: 5.6156e-04 - precision_m: 0.0048 - recall_m: 2.9833e-04\n",
            "Epoch 00003: val_accuracy improved from 0.13281 to 0.17188, saving model to /content/gdrive/MyDrive/model_best11v2.h5\n",
            "419/419 [==============================] - 663s 2s/step - loss: 2.6025 - accuracy: 0.1109 - f1_m: 5.6156e-04 - precision_m: 0.0048 - recall_m: 2.9833e-04 - val_loss: 2.4136 - val_accuracy: 0.1719 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - lr: 1.0000e-04\n",
            "Epoch 4/100\n",
            "419/419 [==============================] - ETA: 0s - loss: 2.5519 - accuracy: 0.1234 - f1_m: 2.8078e-04 - precision_m: 0.0024 - recall_m: 1.4916e-04\n",
            "Epoch 00004: val_accuracy did not improve from 0.17188\n",
            "419/419 [==============================] - 659s 2s/step - loss: 2.5519 - accuracy: 0.1234 - f1_m: 2.8078e-04 - precision_m: 0.0024 - recall_m: 1.4916e-04 - val_loss: 2.3729 - val_accuracy: 0.1641 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - lr: 1.0000e-04\n",
            "Epoch 5/100\n",
            "419/419 [==============================] - ETA: 0s - loss: 2.5010 - accuracy: 0.1489 - f1_m: 0.0014 - precision_m: 0.0119 - recall_m: 7.4582e-04\n",
            "Epoch 00005: val_accuracy improved from 0.17188 to 0.19531, saving model to /content/gdrive/MyDrive/model_best11v2.h5\n",
            "419/419 [==============================] - 662s 2s/step - loss: 2.5010 - accuracy: 0.1489 - f1_m: 0.0014 - precision_m: 0.0119 - recall_m: 7.4582e-04 - val_loss: 2.3344 - val_accuracy: 0.1953 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - lr: 1.0000e-04\n",
            "Epoch 6/100\n",
            "419/419 [==============================] - ETA: 0s - loss: 2.4533 - accuracy: 0.1695 - f1_m: 0.0014 - precision_m: 0.0119 - recall_m: 7.4582e-04\n",
            "Epoch 00006: val_accuracy improved from 0.19531 to 0.22656, saving model to /content/gdrive/MyDrive/model_best11v2.h5\n",
            "419/419 [==============================] - 664s 2s/step - loss: 2.4533 - accuracy: 0.1695 - f1_m: 0.0014 - precision_m: 0.0119 - recall_m: 7.4582e-04 - val_loss: 2.3005 - val_accuracy: 0.2266 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - lr: 1.0000e-04\n",
            "Epoch 7/100\n",
            "419/419 [==============================] - ETA: 0s - loss: 2.4178 - accuracy: 0.1928 - f1_m: 0.0011 - precision_m: 0.0095 - recall_m: 5.9666e-04\n",
            "Epoch 00007: val_accuracy improved from 0.22656 to 0.24219, saving model to /content/gdrive/MyDrive/model_best11v2.h5\n",
            "419/419 [==============================] - 664s 2s/step - loss: 2.4178 - accuracy: 0.1928 - f1_m: 0.0011 - precision_m: 0.0095 - recall_m: 5.9666e-04 - val_loss: 2.2645 - val_accuracy: 0.2422 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - lr: 1.0000e-04\n",
            "Epoch 8/100\n",
            "419/419 [==============================] - ETA: 0s - loss: 2.3699 - accuracy: 0.2184 - f1_m: 0.0042 - precision_m: 0.0334 - recall_m: 0.0022\n",
            "Epoch 00008: val_accuracy improved from 0.24219 to 0.28906, saving model to /content/gdrive/MyDrive/model_best11v2.h5\n",
            "419/419 [==============================] - 665s 2s/step - loss: 2.3699 - accuracy: 0.2184 - f1_m: 0.0042 - precision_m: 0.0334 - recall_m: 0.0022 - val_loss: 2.2293 - val_accuracy: 0.2891 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - lr: 1.0000e-04\n",
            "Epoch 9/100\n",
            "419/419 [==============================] - ETA: 0s - loss: 2.3359 - accuracy: 0.2275 - f1_m: 0.0059 - precision_m: 0.0489 - recall_m: 0.0031\n",
            "Epoch 00009: val_accuracy improved from 0.28906 to 0.34375, saving model to /content/gdrive/MyDrive/model_best11v2.h5\n",
            "419/419 [==============================] - 665s 2s/step - loss: 2.3359 - accuracy: 0.2275 - f1_m: 0.0059 - precision_m: 0.0489 - recall_m: 0.0031 - val_loss: 2.1938 - val_accuracy: 0.3438 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - lr: 1.0000e-04\n",
            "Epoch 10/100\n",
            "419/419 [==============================] - ETA: 0s - loss: 2.2986 - accuracy: 0.2581 - f1_m: 0.0104 - precision_m: 0.0859 - recall_m: 0.0055\n",
            "Epoch 00010: val_accuracy improved from 0.34375 to 0.35156, saving model to /content/gdrive/MyDrive/model_best11v2.h5\n",
            "419/419 [==============================] - 665s 2s/step - loss: 2.2986 - accuracy: 0.2581 - f1_m: 0.0104 - precision_m: 0.0859 - recall_m: 0.0055 - val_loss: 2.1785 - val_accuracy: 0.3516 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - lr: 1.0000e-04\n",
            "Epoch 11/100\n",
            "419/419 [==============================] - ETA: 0s - loss: 2.2520 - accuracy: 0.2838 - f1_m: 0.0174 - precision_m: 0.1456 - recall_m: 0.0092\n",
            "Epoch 00011: val_accuracy did not improve from 0.35156\n",
            "419/419 [==============================] - 661s 2s/step - loss: 2.2520 - accuracy: 0.2838 - f1_m: 0.0174 - precision_m: 0.1456 - recall_m: 0.0092 - val_loss: 2.1499 - val_accuracy: 0.3438 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - lr: 1.0000e-04\n",
            "Epoch 12/100\n",
            " 57/419 [===>..........................] - ETA: 9:26 - loss: 2.2269 - accuracy: 0.2851 - f1_m: 0.0201 - precision_m: 0.1316 - recall_m: 0.0110"
          ]
        }
      ],
      "source": [
        "learning_rate = 1e-4\n",
        "\n",
        "optimizer = tfa.optimizers.RectifiedAdam(learning_rate = learning_rate)\n",
        "\n",
        "#model.compile(optimizer = optimizer, \n",
        "              #loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing = 0.2), \n",
        "              #metrics = ['accuracy'])\n",
        "\n",
        "model.compile(optimizer='sgd', loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing = 0.2),\n",
        "           metrics=['accuracy', f1_m,precision_m, recall_m])\n",
        "\n",
        "lrate = tf.keras.callbacks.LearningRateScheduler(step_decay1)\n",
        "\n",
        "\n",
        "STEP_SIZE_TRAIN = train_set.n // train_set.batch_size\n",
        "STEP_SIZE_VALID = val_set.n // val_set.batch_size\n",
        "\n",
        "#reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_accuracy',\n",
        "                                                 #factor = 0.2,\n",
        "                                                 #patience = 2,\n",
        "                                                 #verbose = 1,\n",
        "                                                 #min_delta = 1e-4,\n",
        "                                                 #min_lr = 1e-6,\n",
        "                                                 #mode = 'max')\n",
        "\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy',\n",
        "                                                 min_delta = 1e-4,\n",
        "                                                 patience = 3,\n",
        "                                                 mode = 'max',\n",
        "                                                 restore_best_weights = True,\n",
        "                                                 verbose = 1)\n",
        "\n",
        "checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath = '/content/gdrive/MyDrive/model_best11v2.h5',\n",
        "                                                  monitor = 'val_accuracy', \n",
        "                                                  verbose = 1, \n",
        "                                                  save_best_only = True,\n",
        "                                                  save_weights_only = True,\n",
        "                                                  mode = 'max')\n",
        "\n",
        "#callbacks = [earlystopping, reduce_lr, checkpointer]\n",
        "\n",
        "callbacks = [earlystopping, lrate, checkpointer]\n",
        "\n",
        "model.fit(x = train_set,\n",
        "          steps_per_epoch = STEP_SIZE_TRAIN,\n",
        "          validation_data = val_set,\n",
        "          validation_steps = STEP_SIZE_VALID,\n",
        "          epochs = EPOCHS,\n",
        "          callbacks = callbacks)\n",
        "\n",
        "model.load_weights('/content/gdrive/MyDrive/model_best11v2.h5')\n",
        "\n",
        "learning_rate = 1e-6\n",
        "\n",
        "lrate2 = tf.keras.callbacks.LearningRateScheduler(step_decay2)\n",
        "\n",
        "#descongela as camadas\n",
        "for layer in range(len(vit_model.layers)):\n",
        "    vit_model.layers[layer].treinable=True\n",
        "\n",
        "  \n",
        "\n",
        "#model.compile(optimizer = optimizer, \n",
        "              #loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing = 0.2), \n",
        "              #metrics = ['accuracy'])\n",
        "\n",
        "model.compile(optimizer='sgd', loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing = 0.2),\n",
        "           metrics=['accuracy', f1_m,precision_m, recall_m])\n",
        "\n",
        "callbacks = [earlystopping, lrate2, checkpointer]\n",
        "\n",
        "model.fit(x = train_set,\n",
        "          steps_per_epoch = STEP_SIZE_TRAIN,\n",
        "          validation_data = val_set,\n",
        "          validation_steps = STEP_SIZE_VALID,\n",
        "          epochs = EPOCHS,\n",
        "          callbacks = callbacks)\n",
        "\n",
        "model.load_weights('/content/gdrive/MyDrive/model_best11v2.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P03ty_iOuIXc"
      },
      "outputs": [],
      "source": [
        "classes={\n",
        "    'Codega':'CD',\n",
        "    'Malvasia Fina':'MF',\n",
        "    'Malvasia Preta':'MP',\n",
        "    'Malvasia Rei':'MR',\n",
        "    'Moscatel Galego':'MG',\n",
        "    'Mourisco Tinto':'MT',\n",
        "    'Rabigato':'RG',\n",
        "    'Tinta Amarela':'TA',\n",
        "    'Tinta Barroca':'TB',\n",
        "    'Tinta Roriz':'TR',\n",
        "    'Tinto Cao':'TC',\n",
        "    'Touriga Nacional':'TN'\n",
        "}\n",
        "\n",
        "model.load_weights('/content/gdrive/MyDrive/model_best11v2.h5')\n",
        "from sklearn import metrics\n",
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "def confusion_matrix(test_data_generator, model):\n",
        "  test_data_generator.reset()\n",
        "  predictions = model.predict(test_data_generator, steps=test_set.samples)\n",
        "  # Get most likely class\n",
        "  predicted_classes = np.argmax(predictions, axis=1)\n",
        "  true_classes = test_data_generator.classes\n",
        "  class_labels = list(test_data_generator.class_indices.keys())\n",
        "  print(class_labels)\n",
        "  class_labels = [classes[x] for x in class_labels]  \n",
        "\n",
        "  report = metrics.classification_report(true_classes, predicted_classes, target_names=class_labels)\n",
        "  cm = metrics.confusion_matrix(true_classes, predicted_classes)\n",
        "  print(report)\n",
        "  plot_confusion_matrix(cm, class_labels)\n",
        "\n",
        "\n",
        "confusion_matrix(test_set, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YilmpsdwXhWI"
      },
      "outputs": [],
      "source": [
        "dot_img_file = 'model_1.png'\n",
        "tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)\n",
        "\n",
        "model.summary()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "[ViT]_DS12_Experimento_5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
